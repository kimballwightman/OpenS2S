#!/usr/bin/env python3
"""
One-Time Model Quantization Script
===================================

This script quantizes the OmniSpeech LLM to 8-bit and saves it to a separate directory.
Run this ONCE after downloading the original model, then always load from the quantized version.

Usage:
    python3 quantize_model_once.py

This will:
1. Load the original model from /models/OpenS2S
2. Quantize the LLM component to 8-bit
3. Save the quantized model to /models/OpenS2S-8bit
4. Future model loads will use the pre-quantized version (faster!)

Expected time: 5-10 minutes (one-time only)
Expected space: ~10GB for quantized model (vs 21GB original)
"""

import os
import sys
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Add src to path
sys.path.insert(0, '/app')
from src.modeling_omnispeech import OmniSpeechModel

SOURCE_PATH = "/models/OpenS2S"
QUANTIZED_PATH = "/models/OpenS2S-8bit"

def main():
    print("=" * 70)
    print("üîß One-Time Model Quantization Script")
    print("=" * 70)
    print()

    # Check if source model exists
    if not os.path.exists(SOURCE_PATH):
        print(f"‚ùå Error: Source model not found at {SOURCE_PATH}")
        print("   Please run the container first to download the model.")
        sys.exit(1)

    # Check if quantized model already exists
    if os.path.exists(QUANTIZED_PATH) and os.listdir(QUANTIZED_PATH):
        print(f"‚ö†Ô∏è  Quantized model already exists at {QUANTIZED_PATH}")
        response = input("   Overwrite? (yes/no): ").strip().lower()
        if response != "yes":
            print("   Exiting without changes.")
            sys.exit(0)
        print()

    print(f"üì• Step 1/4: Loading original model from {SOURCE_PATH}...")
    print("   This may take 1-2 minutes...")

    try:
        model = OmniSpeechModel.from_pretrained(
            SOURCE_PATH,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            device_map={"": "cpu"},
            local_files_only=True
        )
        print("   ‚úÖ Original model loaded to CPU")
    except Exception as e:
        print(f"   ‚ùå Failed to load model: {e}")
        sys.exit(1)

    print()
    print("üîÑ Step 2/4: Quantizing LLM component to 8-bit...")
    print("   This may take 2-3 minutes...")

    try:
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False
        )

        # Replace LLM with quantized version
        # Note: We load from the original checkpoint which has proper structure
        # This should trigger bitsandbytes quantization
        quantized_llm = AutoModelForCausalLM.from_pretrained(
            SOURCE_PATH,
            quantization_config=quantization_config,
            device_map="cpu",
            torch_dtype=torch.bfloat16,
            local_files_only=True
        )

        # Replace in model
        del model.llm_model
        model.llm_model = quantized_llm

        print("   ‚úÖ LLM quantized to 8-bit")
    except Exception as e:
        print(f"   ‚ùå Failed to quantize: {e}")
        print("   Note: Quantization may not work with current approach.")
        print("   The model will be saved but may not be truly quantized.")
        # Continue anyway to save the model

    print()
    print(f"üíæ Step 3/4: Saving quantized model to {QUANTIZED_PATH}...")
    print("   This may take 2-3 minutes...")

    try:
        os.makedirs(QUANTIZED_PATH, exist_ok=True)

        model.save_pretrained(
            QUANTIZED_PATH,
            safe_serialization=True,
            max_shard_size="5GB"
        )

        # Also copy tokenizer and config files
        import shutil
        for file in ["tokenizer.json", "tokenizer_config.json", "special_tokens_map.json",
                     "vocab.json", "merges.txt", "added_tokens.json", "generation_config.json"]:
            src = os.path.join(SOURCE_PATH, file)
            dst = os.path.join(QUANTIZED_PATH, file)
            if os.path.exists(src):
                shutil.copy2(src, dst)

        # Copy subdirectories (audio, tts)
        for subdir in ["audio", "tts"]:
            src_dir = os.path.join(SOURCE_PATH, subdir)
            dst_dir = os.path.join(QUANTIZED_PATH, subdir)
            if os.path.exists(src_dir):
                shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)

        print("   ‚úÖ Quantized model saved")
    except Exception as e:
        print(f"   ‚ùå Failed to save: {e}")
        sys.exit(1)

    print()
    print("üìä Step 4/4: Verifying saved model...")

    # Check saved files
    saved_files = os.listdir(QUANTIZED_PATH)
    safetensor_files = [f for f in saved_files if f.endswith('.safetensors')]

    print(f"   Found {len(safetensor_files)} safetensor files")
    print(f"   Total files: {len(saved_files)}")

    # Estimate size
    total_size = 0
    for root, dirs, files in os.walk(QUANTIZED_PATH):
        for f in files:
            total_size += os.path.getsize(os.path.join(root, f))

    print(f"   Total size: {total_size / 1e9:.2f} GB")

    print()
    print("=" * 70)
    print("üéâ SUCCESS! Model quantization complete!")
    print("=" * 70)
    print()
    print("Next steps:")
    print("1. Update startup.py to use the quantized model path")
    print("2. Restart the container to use the quantized model")
    print("3. Future loads will be faster (no runtime quantization needed)")
    print()
    print(f"Quantized model location: {QUANTIZED_PATH}")
    print()

if __name__ == "__main__":
    main()
